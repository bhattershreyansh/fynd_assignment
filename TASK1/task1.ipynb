{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c6e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e568c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.genai as genai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05125d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini client initialized successfully!\n",
      "Using model: gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "print(\"Gemini client initialized successfully!\")\n",
    "print(\"Using model: gemini-2.5-flash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfbc1066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews: 200\n",
      "Columns: ['business_id', 'date', 'review_id', 'stars', 'text', 'type', 'user_id', 'cool', 'useful', 'funny']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QVR7dsvBeg8xFt9B-vd1BA</td>\n",
       "      <td>2010-07-22</td>\n",
       "      <td>hwYVJs8Ko4PMjI19QcR57g</td>\n",
       "      <td>4</td>\n",
       "      <td>We got here around midnight last Friday... the...</td>\n",
       "      <td>review</td>\n",
       "      <td>90a6z--_CUrl84aCzZyPsg</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24qSrF_XOrvaHDBy-gLIQg</td>\n",
       "      <td>2012-01-22</td>\n",
       "      <td>0mvthYPKb2ZmKhCADiKSmQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Brought a friend from Louisiana here.  She say...</td>\n",
       "      <td>review</td>\n",
       "      <td>9lJAj_2zCvP2jcEiRjF9oA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>j0Uc-GuOe-x9_N_IK1KPpA</td>\n",
       "      <td>2009-05-09</td>\n",
       "      <td>XJHknNIecha6h0wkBSZB4w</td>\n",
       "      <td>3</td>\n",
       "      <td>Every friday, my dad and I eat here. We order ...</td>\n",
       "      <td>review</td>\n",
       "      <td>0VfJi9Au0rVFVnPKcJpt3Q</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RBiiGw8c7j-0a8nk35JO3w</td>\n",
       "      <td>2010-12-22</td>\n",
       "      <td>z6y3GRpYDqTznVe-0dn--Q</td>\n",
       "      <td>1</td>\n",
       "      <td>My husband and I were really, really disappoin...</td>\n",
       "      <td>review</td>\n",
       "      <td>lwppVF0Yqkuwt-xaEuugqw</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U8VA-RW6LYOhxR-Ygi6eDw</td>\n",
       "      <td>2011-01-17</td>\n",
       "      <td>vhWHdemMvsqVNv5zi2OMiA</td>\n",
       "      <td>5</td>\n",
       "      <td>Love this place!  Was in phoenix 3 weeks for w...</td>\n",
       "      <td>review</td>\n",
       "      <td>Y2R_tlSk4lTHiLXTDsn1rg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  QVR7dsvBeg8xFt9B-vd1BA  2010-07-22  hwYVJs8Ko4PMjI19QcR57g      4   \n",
       "1  24qSrF_XOrvaHDBy-gLIQg  2012-01-22  0mvthYPKb2ZmKhCADiKSmQ      5   \n",
       "2  j0Uc-GuOe-x9_N_IK1KPpA  2009-05-09  XJHknNIecha6h0wkBSZB4w      3   \n",
       "3  RBiiGw8c7j-0a8nk35JO3w  2010-12-22  z6y3GRpYDqTznVe-0dn--Q      1   \n",
       "4  U8VA-RW6LYOhxR-Ygi6eDw  2011-01-17  vhWHdemMvsqVNv5zi2OMiA      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  We got here around midnight last Friday... the...  review   \n",
       "1  Brought a friend from Louisiana here.  She say...  review   \n",
       "2  Every friday, my dad and I eat here. We order ...  review   \n",
       "3  My husband and I were really, really disappoin...  review   \n",
       "4  Love this place!  Was in phoenix 3 weeks for w...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  90a6z--_CUrl84aCzZyPsg     5       5      2  \n",
       "1  9lJAj_2zCvP2jcEiRjF9oA     0       0      0  \n",
       "2  0VfJi9Au0rVFVnPKcJpt3Q     0       0      0  \n",
       "3  lwppVF0Yqkuwt-xaEuugqw     2       2      2  \n",
       "4  Y2R_tlSk4lTHiLXTDsn1rg     0       1      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv('yelp.csv') \n",
    "df_sample = df.sample(n=200, random_state=42).reset_index(drop=True)\n",
    "print(f\"Total reviews: {len(df_sample)}\")\n",
    "print(f\"Columns: {df_sample.columns.tolist()}\")\n",
    "df_sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996c529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROMPT_1_TEMPLATE = \"\"\"You are a rating prediction system. Based on the review text below, predict the star rating (1-5).\n",
    "\n",
    "Review: \"{review_text}\"\n",
    "\n",
    "Return ONLY a JSON object in this exact format:\n",
    "{{\"predicted_stars\": <number>, \"explanation\": \"<brief reason>\"}}\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_2_TEMPLATE = \"\"\"Analyze the following review step-by-step:\n",
    "\n",
    "Review: \"{review_text}\"\n",
    "\n",
    "Steps:\n",
    "1. Identify the sentiment (positive, negative, neutral, mixed)\n",
    "2. Look for specific indicators (complaints, praise, specific issues, enthusiasm level)\n",
    "3. Based on these factors, determine the star rating (1-5)\n",
    "\n",
    "Return ONLY a JSON object:\n",
    "{{\"predicted_stars\": <number>, \"explanation\": \"<reasoning based on sentiment and indicators>\"}}\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_3_TEMPLATE = \"\"\"You are an expert at predicting star ratings from reviews. Here are examples:\n",
    "\n",
    "Example 1:\n",
    "Review: \"Absolutely amazing food! Best pizza I've ever had. Service was fantastic too.\"\n",
    "Output: {{\"predicted_stars\": 5, \"explanation\": \"Highly positive language with superlatives indicating excellent experience\"}}\n",
    "\n",
    "Example 2:\n",
    "Review: \"Food was okay, nothing special. Service took forever.\"\n",
    "Output: {{\"predicted_stars\": 2, \"explanation\": \"Mediocre food quality combined with poor service indicates below average experience\"}}\n",
    "\n",
    "Example 3:\n",
    "Review: \"Good food and decent prices. Could be better but satisfied overall.\"\n",
    "Output: {{\"predicted_stars\": 4, \"explanation\": \"Positive with minor reservations suggests good but not perfect experience\"}}\n",
    "\n",
    "Now predict for this review:\n",
    "Review: \"{review_text}\"\n",
    "\n",
    "Return ONLY a JSON object:\n",
    "{{\"predicted_stars\": <number>, \"explanation\": \"<brief reasoning>\"}}\"\"\"\n",
    "\n",
    "prompts = {\n",
    "    \"Prompt 1 (Basic)\": PROMPT_1_TEMPLATE,\n",
    "    \"Prompt 2 (Chain-of-Thought)\": PROMPT_2_TEMPLATE,\n",
    "    \"Prompt 3 (Few-Shot)\": PROMPT_3_TEMPLATE\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a796830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(review_text, prompt_template, max_retries=2):\n",
    "    \"\"\"Call Gemini API and return parsed JSON response\"\"\"\n",
    "    prompt = prompt_template.format(review_text=review_text)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            response_text = response.text.strip()\n",
    "            \n",
    "            if \"```json\" in response_text:\n",
    "                response_text = response_text.split(\"```json\").split(\"```\").strip()[1]\n",
    "            elif \"```\" in response_text:\n",
    "                response_text = response_text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "            \n",
    "            result = json.loads(response_text)\n",
    "    \n",
    "            if \"predicted_stars\" in result and \"explanation\" in result:\n",
    "                return {\n",
    "                    \"predicted_stars\": int(result[\"predicted_stars\"]),\n",
    "                    \"explanation\": result[\"explanation\"],\n",
    "                    \"is_valid\": True,\n",
    "                    \"raw_response\": response_text\n",
    "                }\n",
    "            else:\n",
    "                return {\"is_valid\": False, \"error\": \"Missing required fields\", \"raw_response\": response_text}\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\"is_valid\": False, \"error\": f\"JSON parse error: {str(e)}\", \"raw_response\": response.text}\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\"is_valid\": False, \"error\": str(e), \"raw_response\": str(e)}\n",
    "        \n",
    "        time.sleep(1) \n",
    "    \n",
    "    return {\"is_valid\": False, \"error\": \"Max retries exceeded\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dee327a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions):\n",
    "    '''Calculate accuracy and other metrics from predictions'''\n",
    "    valid_predictions = [p for p in predictions if p['is_valid']]\n",
    "    \n",
    "    if not valid_predictions:\n",
    "        return {\n",
    "            'accuracy': 0,\n",
    "            'valid_count': 0,\n",
    "            'total_count': len(predictions),\n",
    "            'json_validity_rate': 0\n",
    "        }\n",
    "    \n",
    "    actual = [p['actual_stars'] for p in valid_predictions]\n",
    "    predicted = [p['predicted_stars'] for p in valid_predictions]\n",
    "    \n",
    "    accuracy = accuracy_score(actual, predicted)\n",
    "    conf_matrix = confusion_matrix(actual, predicted, labels=[1, 2, 3, 4, 5])\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'valid_count': len(valid_predictions),\n",
    "        'total_count': len(predictions),\n",
    "        'json_validity_rate': len(valid_predictions) / len(predictions),\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'actual': actual,\n",
    "        'predicted': predicted\n",
    "    }\n",
    "\n",
    "def calculate_consistency(predictions):\n",
    "    '''Calculate prediction consistency (std dev of errors)'''\n",
    "    valid_predictions = [p for p in predictions if p['is_valid']]\n",
    "    if not valid_predictions:\n",
    "        return 0\n",
    "    \n",
    "    errors = [abs(p['predicted_stars'] - p['actual_stars']) for p in valid_predictions]\n",
    "    return np.std(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e43b985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing: Prompt 1 (Basic)\n",
      "============================================================\n",
      "Processed 20/200 reviews...\n",
      "Processed 40/200 reviews...\n",
      "Processed 60/200 reviews...\n",
      "Processed 80/200 reviews...\n",
      "Processed 100/200 reviews...\n",
      "Processed 120/200 reviews...\n",
      "Processed 140/200 reviews...\n",
      "Processed 160/200 reviews...\n",
      "Processed 180/200 reviews...\n",
      "Processed 200/200 reviews...\n",
      "Completed! Valid JSON responses: 0/200\n",
      "\n",
      "============================================================\n",
      "Testing: Prompt 2 (Chain-of-Thought)\n",
      "============================================================\n",
      "Processed 20/200 reviews...\n",
      "Processed 40/200 reviews...\n",
      "Processed 60/200 reviews...\n",
      "Processed 80/200 reviews...\n",
      "Processed 100/200 reviews...\n",
      "Processed 120/200 reviews...\n",
      "Processed 140/200 reviews...\n",
      "Processed 160/200 reviews...\n",
      "Processed 180/200 reviews...\n",
      "Processed 200/200 reviews...\n",
      "Completed! Valid JSON responses: 0/200\n",
      "\n",
      "============================================================\n",
      "Testing: Prompt 3 (Few-Shot)\n",
      "============================================================\n",
      "Processed 20/200 reviews...\n",
      "Processed 40/200 reviews...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m review_text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Adjust column name if needed\u001b[39;00m\n\u001b[0;32m     13\u001b[0m actual_stars \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstars\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Adjust column name if needed\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_rating\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactual_stars\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m actual_stars\n\u001b[0;32m     18\u001b[0m predictions\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[1;32mIn[5], line 34\u001b[0m, in \u001b[0;36mpredict_rating\u001b[1;34m(review_text, prompt_template, max_retries)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m attempt \u001b[38;5;241m==\u001b[39m max_retries \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     32\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_valid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(e), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_response\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(e)}\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_valid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax retries exceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for prompt_name, prompt_template in prompts.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {prompt_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    predictions = []\n",
    "    valid_count = 0\n",
    "    \n",
    "    for idx, row in df_sample.iterrows():\n",
    "        review_text = row['text']  # Adjust column name if needed\n",
    "        actual_stars = row['stars']  # Adjust column name if needed\n",
    "        \n",
    "        result = predict_rating(review_text, prompt_template)\n",
    "        \n",
    "        result['actual_stars'] = actual_stars\n",
    "        predictions.append(result)\n",
    "        \n",
    "        if result['is_valid']:\n",
    "            valid_count += 1\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (idx + 1) % 20 == 0:\n",
    "            print(f\"Processed {idx + 1}/{len(df_sample)} reviews...\")\n",
    "        \n",
    "        time.sleep(0.5)  # Rate limiting for free tier\n",
    "    \n",
    "    results[prompt_name] = predictions\n",
    "    print(f\"Completed! Valid JSON responses: {valid_count}/{len(df_sample)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gemini client initialized successfully!\n",
      "✓ Using model: gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics_summary = {}\n",
    "\n",
    "for prompt_name, predictions in results.items():\n",
    "    metrics = calculate_metrics(predictions)\n",
    "    consistency = calculate_consistency(predictions)\n",
    "    \n",
    "    metrics_summary[prompt_name] = {\n",
    "        'Accuracy': f\"{metrics['accuracy']:.2%}\",\n",
    "        'JSON Validity Rate': f\"{metrics['json_validity_rate']:.2%}\",\n",
    "        'Valid Predictions': f\"{metrics['valid_count']}/{metrics['total_count']}\",\n",
    "        'Consistency (Lower is Better)': f\"{consistency:.3f}\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{prompt_name} - Results\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.2%}\")\n",
    "    print(f\"JSON Validity: {metrics['json_validity_rate']:.2%}\")\n",
    "    print(f\"Valid Predictions: {metrics['valid_count']}/{metrics['total_count']}\")\n",
    "    print(f\"Consistency (Std Dev of Errors): {consistency:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c1dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(metrics_summary).T\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON TABLE - All Prompts\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Save results\n",
    "comparison_df.to_csv('prompt_comparison_results.csv')\n",
    "print(\"Results saved to 'prompt_comparison_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Accuracy Comparison\n",
    "prompt_names = list(metrics_summary.keys())\n",
    "accuracies = [float(metrics_summary[p]['Accuracy'].strip('%'))/100 for p in prompt_names]\n",
    "axes[0, 0].bar(range(len(prompt_names)), accuracies, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "axes[0, 0].set_xticks(range(len(prompt_names)))\n",
    "axes[0, 0].set_xticklabels(prompt_names, rotation=15, ha='right')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Accuracy Comparison Across Prompts')\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0, 0].text(i, v + 0.02, f'{v:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. JSON Validity Rate\n",
    "validity_rates = [float(metrics_summary[p]['JSON Validity Rate'].strip('%'))/100 for p in prompt_names]\n",
    "axes[0, 1].bar(range(len(prompt_names)), validity_rates, color=['#9b59b6', '#f39c12', '#1abc9c'])\n",
    "axes[0, 1].set_xticks(range(len(prompt_names)))\n",
    "axes[0, 1].set_xticklabels(prompt_names, rotation=15, ha='right')\n",
    "axes[0, 1].set_ylabel('JSON Validity Rate')\n",
    "axes[0, 1].set_title('JSON Validity Rate Comparison')\n",
    "axes[0, 1].set_ylim([0, 1])\n",
    "for i, v in enumerate(validity_rates):\n",
    "    axes[0, 1].text(i, v + 0.02, f'{v:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Confusion Matrix for Best Performing Prompt\n",
    "best_prompt = max(metrics_summary.keys(), key=lambda x: float(metrics_summary[x]['Accuracy'].strip('%')))\n",
    "best_metrics = calculate_metrics(results[best_prompt])\n",
    "sns.heatmap(best_metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5], ax=axes[1, 0])\n",
    "axes[1, 0].set_xlabel('Predicted Stars')\n",
    "axes[1, 0].set_ylabel('Actual Stars')\n",
    "axes[1, 0].set_title(f'Confusion Matrix - {best_prompt}')\n",
    "\n",
    "# 4. Error Distribution\n",
    "all_errors = []\n",
    "all_labels = []\n",
    "for prompt_name in prompt_names:\n",
    "    valid_preds = [p for p in results[prompt_name] if p['is_valid']]\n",
    "    errors = [abs(p['predicted_stars'] - p['actual_stars']) for p in valid_preds]\n",
    "    all_errors.extend(errors)\n",
    "    all_labels.extend([prompt_name] * len(errors))\n",
    "\n",
    "error_df = pd.DataFrame({'Prompt': all_labels, 'Absolute Error': all_errors})\n",
    "sns.boxplot(data=error_df, x='Prompt', y='Absolute Error', ax=axes[1, 1])\n",
    "axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=15, ha='right')\n",
    "axes[1, 1].set_title('Error Distribution Across Prompts')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prompt_evaluation_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved to 'prompt_evaluation_results.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a678d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED ANALYSIS & DISCUSSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n### PROMPT DESIGN RATIONALE ###\\n\")\n",
    "\n",
    "print(\"1. PROMPT 1 (Basic):\")\n",
    "print(\"   - Simple, direct instruction\")\n",
    "print(\"   - Minimal guidance to the model\")\n",
    "print(\"   - Tests baseline performance\")\n",
    "print(\"   - Expected: Fast but potentially less accurate\")\n",
    "\n",
    "print(\"\\n2. PROMPT 2 (Chain-of-Thought):\")\n",
    "print(\"   - Guides model through reasoning steps\")\n",
    "print(\"   - Explicitly asks to identify sentiment and indicators\")\n",
    "print(\"   - Expected: Better reasoning, potentially higher accuracy\")\n",
    "print(\"   - Trade-off: Slightly longer processing time\")\n",
    "\n",
    "print(\"\\n3. PROMPT 3 (Few-Shot):\")\n",
    "print(\"   - Provides concrete examples of rating patterns\")\n",
    "print(\"   - Shows model what good predictions look like\")\n",
    "print(\"   - Expected: Most consistent and accurate\")\n",
    "print(\"   - Trade-off: Longer prompt = higher token usage\")\n",
    "\n",
    "print(\"\\n### RESULTS SUMMARY ###\\n\")\n",
    "\n",
    "# Find best performing prompt for each metric\n",
    "best_accuracy = max(metrics_summary.keys(), key=lambda x: float(metrics_summary[x]['Accuracy'].strip('%')))\n",
    "best_validity = max(metrics_summary.keys(), key=lambda x: float(metrics_summary[x]['JSON Validity Rate'].strip('%')))\n",
    "best_consistency = min(metrics_summary.keys(), key=lambda x: float(metrics_summary[x]['Consistency (Lower is Better)']))\n",
    "\n",
    "print(f\"Best Accuracy: {best_accuracy} ({metrics_summary[best_accuracy]['Accuracy']})\")\n",
    "print(f\"Best JSON Validity: {best_validity} ({metrics_summary[best_validity]['JSON Validity Rate']})\")\n",
    "print(f\"Best Consistency: {best_consistency} ({metrics_summary[best_consistency]['Consistency (Lower is Better)']})\")\n",
    "\n",
    "print(\"\\n### KEY FINDINGS ###\\n\")\n",
    "print(\"1. Accuracy: How well each prompt predicted the correct star rating\")\n",
    "print(\"2. JSON Validity: How reliably each prompt returned properly formatted JSON\")\n",
    "print(\"3. Consistency: How stable the predictions are (lower std dev = more reliable)\")\n",
    "\n",
    "print(\"\\n### TRADE-OFFS ###\\n\")\n",
    "print(\"- Basic Prompt: Fast, simple, but may lack nuance\")\n",
    "print(\"- Chain-of-Thought: Better reasoning, but requires more tokens\")\n",
    "print(\"- Few-Shot: Most accurate, but highest token cost and prompt complexity\")\n",
    "\n",
    "print(\"\\n### RECOMMENDATIONS ###\\n\")\n",
    "if best_accuracy == best_validity == best_consistency:\n",
    "    print(f\"✓ {best_accuracy} is the clear winner across all metrics\")\n",
    "else:\n",
    "    print(\"✓ Choose based on priority:\")\n",
    "    print(f\"  - For accuracy: {best_accuracy}\")\n",
    "    print(f\"  - For reliability: {best_validity}\")\n",
    "    print(f\"  - For consistency: {best_consistency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddbacde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('detailed_predictions.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\nAll results saved!\")\n",
    "print(\"- prompt_comparison_results.csv\")\n",
    "print(\"- prompt_evaluation_results.png\")\n",
    "print(\"- detailed_predictions.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
